# ğŸ’¬ RAG-Powered Question Answering System using LLMs and Vector Databases

This project demonstrates how **knowledge workers** can leverage **Large Language Models (LLMs)** with **Retrieval-Augmented Generation (RAG)** and **Vector Databases** (like FAISS/Chroma) to extract meaningful answers from a large set of documents.

---

## ğŸš€ Features

- ğŸ” **Semantic Search**: Embed and store documents as vectors for efficient and relevant search
- ğŸ§  **RAG Pipeline**: Retrieve context from a vector DB and generate answers using LLMs
- ğŸ§° **Embeddings Support**: Includes `OpenAIEmbeddings` and `HuggingFaceEmbeddings`
- ğŸ“š **Use Case**: Tailored for knowledge workers â€“ analysts, product managers, legal, etc.
- ğŸ§ª **Interactive Q&A**: Ask natural language questions and get concise, context-aware answers

---

## ğŸ—ï¸ Tech Stack

| Component       | Description                                        |
|-----------------|----------------------------------------------------|
| **LLM**         | OpenAI GPT / HuggingFace models                    |
| **RAG**         | Retrieval-Augmented Generation                     |
| **Vector DB**   | FAISS or Chroma (configurable)                     |
| **Embeddings**  | OpenAI / SentenceTransformers from Hugging Face   |
| **LangChain**   | Framework to orchestrate document loading, indexing, and QA |

---

---

## ğŸ“¥ Installation

1. Clone this repo:
   ```bash
   git clone https://github.com/Atharvax16/AI-Knowledge-Worker-using-RAG.git
   cd AI-Knowledge-Worker-using-RAG
